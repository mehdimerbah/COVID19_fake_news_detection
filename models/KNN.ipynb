{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMsVhxi5rh/1MRGtvmldaby",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mehdimerbah/COVID19_fake_news_detection/blob/main/models/KNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hT_fcbnb1_Gv"
      },
      "source": [
        "# Importing Libraries and Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUW1cH-gzgrv"
      },
      "source": [
        "from math import sqrt\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "import json"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uw6CgX-i-Ser"
      },
      "source": [
        "## IMPORT DATA HERE\n",
        "dataset = pd.read_csv('https://raw.githubusercontent.com/mehdimerbah/COVID19_fake_news_detection/main/preprocessing/whole_processed_data.csv')\n",
        "\n",
        "## Extract Features\n",
        "count_vectorizer = CountVectorizer()\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "vectorized = count_vectorizer.fit_transform(dataset['tweet'])\n",
        "tfidf_transformed = tfidf_transformer.fit_transform(vectorized)\n",
        "\n",
        "## Wrangling\n",
        "transformed_dataset = pd.DataFrame(tfidf_transformed.toarray())\n",
        "transformed_dataset['label'] = dataset['label']\n",
        "transformed_dataset.head()\n",
        "\n",
        "## Splitting to Training, Validation and Testing\n",
        "training_set = transformed_dataset.iloc[:6420]\n",
        "validation_set = transformed_dataset.iloc[6420:8560]\n",
        "testing_set = transformed_dataset.iloc[8560:]\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUIC6Vyh3aEM"
      },
      "source": [
        "# Buidling the Model\n",
        "K-Nearest Neighbors (KNN) is an unsupervised learning algorithm. It relies on calculating the distance between datapoints, as specified by a certain method (Mnhattan, Euclidean...). Next, it gets the neighbors with minimum distance, ie. those closest to the data point we want to classify.\n",
        "## Calculating Distance\n",
        "For this case we will calculate the Euclidean distance between the rows. Since we have a sparse matrix of normalized count values that we got from out TF-IDF Transformer, we could find the euclidean distance by taking the square-root of the difference squared of each word-count value across all the tweets.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZguCWqy64_6_"
      },
      "source": [
        "def euclidean_distance(tweet0, tweet1):\n",
        "  ## init distance to 0\n",
        "    distance = 0.0\n",
        "    ## loop through the word counts both tweets and take the\n",
        "    ##difference at each position\n",
        "    for i in range(len(tweet0)-1):\n",
        "        distance += (tweet0[i] - tweet1[i])**2\n",
        "    ## return the square-root of the squared distance\n",
        "    return sqrt(distance)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iukjnzcx5ubC"
      },
      "source": [
        "## Getting the Neighbors\n",
        "In this step, we use the distances from the previous step to see the closest k-neighbors to our datapoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7prqb_-H5riH"
      },
      "source": [
        "def get_neighbors(training_set, test_tweet, k):\n",
        "  ## init list of distances to store (tweet, distance) tuples\n",
        "    distances = list()\n",
        "    ## loop through tweets in the dataset and calculate distance to test_tweet\n",
        "    for tweet in training_set:\n",
        "        dist = euclidean_distance(test_tweet, tweet)\n",
        "        distances.append((tweet, dist))\n",
        "    ## sort the (tweet, distance) entries based on distance\n",
        "    distances.sort(key=lambda entry: entry[1])\n",
        "    ## Now we can get the neighbors based on the specified k\n",
        "    neighbors = list()\n",
        "    for i in range(k):\n",
        "        neighbors.append(distances[i][0])\n",
        "    return neighbors"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUo-JxEm7V5j"
      },
      "source": [
        "## Class Prediction\n",
        "Now that we have a way to calculate the distance and a method to get the neighbors using that distance measure, we can start making predictions. \n",
        "A prediction of class label `y` is basically the most frequent class of the k-neighbors closest to our test data-point. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccc0ETGM8GXQ"
      },
      "source": [
        "def predict_class(training_set, test_tweet, k):\n",
        "    ## getting the neighbors of our test data point\n",
        "    neighbors = get_neighbors(training_set, test_tweet, k)\n",
        "    ## getting the class labels of the k-neighbors\n",
        "    labels = [row[-1] for row in neighbors]\n",
        "    ## Now we make a prediction based on the most frequent class\n",
        "    prediction = max(set(labels), key=labels.count)\n",
        "    return prediction\n",
        "\n",
        "def k_nearest_neighbors(training_set, testing_set, k):\n",
        "  predictions = list()\n",
        "  for tweet in testing_set:\n",
        "    prediction = predict_class(training_set, tweet, k)\n",
        "    predictions.append(prediction)\n",
        "  return predictions"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lup2Rzg82I3"
      },
      "source": [
        "# Testing on Fake data (Remove later)\n",
        "small test on fake dataset to make sure all is working fine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DC-UWK8G81ho",
        "outputId": "ece8b641-a21e-42e7-d64d-07c49c8186a9"
      },
      "source": [
        "dataset = [[2.7810836,2.550537003,0],\n",
        "    [1.465489372,2.362125076,0],\n",
        "    [3.396561688,4.400293529,0],\n",
        "    [1.38807019,1.850220317,0],\n",
        "    [3.06407232,3.005305973,0],\n",
        "    [7.627531214,2.759262235,1],\n",
        "    [5.332441248,2.088626775,1],\n",
        "    [6.922596716,1.77106367,1],\n",
        "    [8.675418651,-0.242068655,1],\n",
        "    [7.673756466,3.508563011,1]]\n",
        "tweet0 = dataset[0]\n",
        "i = 0\n",
        "for tweet in dataset:\n",
        "  distance = euclidean_distance(tweet0, tweet)\n",
        "  print(\"Distance between tweet0 and tweet%d is %.3f\" % (i,distance))\n",
        "  i+=1    "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distance between tweet0 and tweet0 is 0.000\n",
            "Distance between tweet0 and tweet1 is 1.329\n",
            "Distance between tweet0 and tweet2 is 1.949\n",
            "Distance between tweet0 and tweet3 is 1.559\n",
            "Distance between tweet0 and tweet4 is 0.536\n",
            "Distance between tweet0 and tweet5 is 4.851\n",
            "Distance between tweet0 and tweet6 is 2.593\n",
            "Distance between tweet0 and tweet7 is 4.214\n",
            "Distance between tweet0 and tweet8 is 6.522\n",
            "Distance between tweet0 and tweet9 is 4.986\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEiYZsb99EH8",
        "outputId": "ae1d21c2-3188-4d0b-b275-f56805771335"
      },
      "source": [
        "neighbors = get_neighbors(dataset, tweet0, 3)\n",
        "for neighbor in neighbors:\n",
        "  print(neighbor)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2.7810836, 2.550537003, 0]\n",
            "[3.06407232, 3.005305973, 0]\n",
            "[1.465489372, 2.362125076, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YRu0Rup_H2l",
        "outputId": "f661da06-2b6a-4e08-812f-46c87dce0e65"
      },
      "source": [
        "prediction = predict_class(dataset, tweet0, 3)\n",
        "print('Expected Class: %d\\nGot Class: %d'% (dataset[0][-1], prediction))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected Class: 0\n",
            "Got Class: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YH198lGx_6_0"
      },
      "source": [
        "Seems like it's working!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLR0gS-w3EPa"
      },
      "source": [
        "# Using The Standard Library Classifier \n",
        "Seems like the classifier is working on the data as is. Some better wrangling and splitting needs to be done to make it cleaner."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhuJKP_0hTAp"
      },
      "source": [
        "classifier = KNN(n_neighbors=5)\n",
        "classifier.fit(X=training_set.iloc[:,:18215], y=training_set.iloc[:,18215])\n",
        "predictions = classifier.predict(validation_set.iloc[:,:18215])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ga9K3UlrnX-X"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpOw2_O81pLS",
        "outputId": "02f114fb-5163-4e01-d4e4-dd53a910c768"
      },
      "source": [
        "print(classification_report(validation_set.iloc[:,18215], predictions))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "        fake       0.90      0.91      0.91      1020\n",
            "        real       0.92      0.91      0.91      1120\n",
            "\n",
            "    accuracy                           0.91      2140\n",
            "   macro avg       0.91      0.91      0.91      2140\n",
            "weighted avg       0.91      0.91      0.91      2140\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhqjqortaXsD"
      },
      "source": [
        "def get_metrics(predicted,true):\n",
        "    metrics = dict()\n",
        "    metrics['accuracy'] = round(accuracy_score(predicted, true), 5)\n",
        "    metrics['precision'] = round(precision_score(predicted, true, average = 'weighted'), 5)\n",
        "    metrics['recall'] = round(recall_score(predicted, true, average = 'weighted'), 5)\n",
        "    metrics['f1'] = round(f1_score(predicted, true, average = 'weighted'), 5)\n",
        "    \n",
        "    return metrics\n",
        "    "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZS86Layta901",
        "outputId": "ef4e1ce2-0a29-469e-a33e-1e9edffe071a"
      },
      "source": [
        "metrics = get_metrics(predictions, validation_set['label'])\n",
        "print(metrics)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'accuracy': 0.91121, 'precision': 0.91121, 'recall': 0.91121, 'f1': 0.9112}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyKzpUdkbYVY"
      },
      "source": [
        "with open(\"KNN_results.json\", \"w\") as output:\n",
        "    json.dump(metrics, output)"
      ],
      "execution_count": 19,
      "outputs": []
    }
  ]
}