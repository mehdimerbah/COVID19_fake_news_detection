{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SVM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLp5sPvEfl9U"
      },
      "source": [
        "# Importing Libraries and Data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b85PssibzLCM"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, average_precision_score, precision_recall_curve, f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBO3MspZfuX_"
      },
      "source": [
        "training = pd.read_csv('https://raw.githubusercontent.com/mehdimerbah/COVID19_fake_news_detection/main/preprocessing/processed_training_data.csv')\n",
        "validation = pd.read_csv('https://raw.githubusercontent.com/mehdimerbah/COVID19_fake_news_detection/main/preprocessing/processed_validation_data.csv')\n",
        "testing = pd.read_csv('https://raw.githubusercontent.com/mehdimerbah/COVID19_fake_news_detection/main/preprocessing/processed_testing_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKOVcaTNydta"
      },
      "source": [
        "# Feature Extraction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amvdvC4GuglV"
      },
      "source": [
        "## The Count Vectorizer\n",
        "A **_Count Vectorizer_** is a simple approach to tokenize a collection of text documents and build a dictionary of known words in these documents. It basically takes in a text of collection of texts and then encodes every word with a number. we do this by using the `fit()` function on our text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YucqcgKuiavv",
        "outputId": "fac4fd91-c4d8-4ff2-f6b9-a7043f35a040"
      },
      "source": [
        "count_vectorizer = CountVectorizer()\n",
        "text = ['This is a test to test the vectorizer. We are working on a datamining project project']\n",
        "count_vectorizer.fit(text)\n",
        "print(dict(list(count_vectorizer.vocabulary_.items())[0:10]))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'this': 7, 'is': 2, 'test': 5, 'to': 8, 'the': 6, 'vectorizer': 9, 'we': 10, 'are': 0, 'working': 11, 'on': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-azclZ6xGpp"
      },
      "source": [
        "We can see that we get a dictionary of words and their respective encoding. We say that the `count_vectorizer` has **_learned the text vocabulary_**\n",
        "Now we can use this to transform the text into an array (vector) of word counts. Each word count is stored at the position specified by its unique encoding number we got from the previous step. To do this we use the `transform()` function on the text from our count_vectorizer object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "at4zyOc1x7ei",
        "outputId": "7a36147d-e6f5-4f15-948f-cfbaa68b0cfe"
      },
      "source": [
        "vector = count_vectorizer.transform(text)\n",
        "print(vector.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 1 1 1 2 2 1 1 1 1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ0Rsxm9zl5J"
      },
      "source": [
        "We can see that now we have a vector of wordcounts. All words have a count of **1** except for _test_ and _project_ at positions 4 and 5 respectively that have a count of 2 since they were repeated.\n",
        "\n",
        "Now let's apply this process to our dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lDgtKC40EBA",
        "outputId": "32de0fab-3adb-4b6d-961a-2439f46f8774"
      },
      "source": [
        "count_vectorizer = CountVectorizer()\n",
        "count_vectorizer.fit(training['tweet'])\n",
        "vectorized = count_vectorizer.transform(training['tweet'])\n",
        "print(dict(list(count_vectorizer.vocabulary_.items())[0:8]))\n",
        "print('The transformed data matrix dimensions:', vectorized.shape)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'cdc': 3547, 'currently': 4561, 'reports': 11073, '99031': 1870, 'deaths': 4682, 'general': 6409, 'discrepancies': 5037, 'death': 4681}\n",
            "The transformed data matrix dimensions: (6420, 14122)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nxAXk_e1IEf"
      },
      "source": [
        "So now we have a data matrix for the 6420 (rows) and 14122 distinct words (columns). `Each vectorized[i][j]` entry is a count for the word at the encoded position `j` in the tweet `i`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCfbNugdun3y"
      },
      "source": [
        "## TF-IDF Transformer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELmHLuAN2GRq"
      },
      "source": [
        "The Tern Frequency - Inverse Document Frequency transformer gives us two types of count statsitics about the count in our text: \n",
        "**Term Frequency** summarizes how often a word appears in a text.\n",
        "**Inverse Document Frequency** normalizes the word count by checkin the appearance of the word across all the given texts.\n",
        "We can do this transformation using the `fit`, `transform` function sequence. Normally, when using the transformer alone we would have to do the vectorization first using a `TfidfVectorizer` then get a normalized count matrix; but since we have a count matrix `vectorized`, we can just normalize it directly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JG_b8n8ju9HJ",
        "outputId": "44452911-87b9-4990-f4b1-9e8fde897330"
      },
      "source": [
        "tfidf_transformer = TfidfTransformer()\n",
        "tfidf_transformer.fit(vectorized)\n",
        "transformed_tfidf = tfidf_transformer.transform(vectorized)\n",
        "print(transformed_tfidf.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6420, 14122)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearSVC()"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syqVOtDOOajo"
      },
      "source": [
        "Since we will be repeating these steps for both the validation and training data set, we could store them in a `Pipeline` object and fit them over the data set we want. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucbLxWObGCH2"
      },
      "source": [
        "pipeline = Pipeline([\n",
        "        ('count_vectorizer', CountVectorizer()),  \n",
        "        ('tfidf_transformer', TfidfTransformer()),  \n",
        "        ('classifier', LinearSVC())\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z_NzItlQJet"
      },
      "source": [
        "# Model Fitting and Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR1FD8qrPisL"
      },
      "source": [
        "The `Pipeline.fit()` function runs both `fit()` and `transform()` functions for the `CountVectorizer` and the `TfidfTransformer` and then fits the classifier. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfIEV_8nH6Ea",
        "outputId": "8497772a-b956-460c-9a37-aa5ac1cf97cd"
      },
      "source": [
        "pipeline.fit(training['tweet'], training['label'])\n",
        "prediction = pipeline.predict(validation['tweet'])\n",
        "print(prediction)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['fake' 'real' 'fake' ... 'fake' 'real' 'real']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5F6nLUgLM4Lt",
        "outputId": "deb5cf34-40c3-497d-f666-3d0ea31ab8e8"
      },
      "source": [
        "print(classification_report(validation['label'], prediction))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "        fake       0.92      0.94      0.93      1020\n",
            "        real       0.95      0.93      0.94      1120\n",
            "\n",
            "    accuracy                           0.93      2140\n",
            "   macro avg       0.93      0.94      0.93      2140\n",
            "weighted avg       0.93      0.93      0.93      2140\n",
            "\n"
          ]
        }
      ]
    }
  ]
}